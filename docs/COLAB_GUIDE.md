# ğŸš€ Google Colab å®Œæ•´æŒ‡å—

## å‰ç½®æ¢ä»¶

- Google å¸³è™Ÿ
- è¨ªå• Google Colabï¼šhttps://colab.research.google.com
- å…è²» GPUï¼ˆT4 æˆ– L4ï¼‰

## Step 1ï¼šå»ºç«‹æ–° Notebook ä¸¦å•Ÿç”¨ GPU

1. æ‰“é–‹ https://colab.research.google.com
2. é»æ“Šã€Œæ–°å¢ç­†è¨˜æœ¬ã€
3. å³ä¸Šè§’é»ã€Œâš™ï¸ è¨­å®šã€
4. åœ¨ã€Œç¡¬é«”åŠ é€Ÿå™¨ã€é¸ **GPU (T4 æˆ– L4)**
5. é»ã€Œå„²å­˜ã€

## Step 2ï¼šç’°å¢ƒæª¢æŸ¥ï¼ˆColab Cell 1ï¼‰

è¤‡è£½è²¼ä¸Šæ­¤ä»£ç¢¼ï¼š

```python
# æª¢æŸ¥ GPU ç‹€æ…‹
import torch

print("\n" + "="*60)
print("ğŸ”§ Environment Check")
print("="*60)

print(f"\nâœ… PyTorch Version: {torch.__version__}")
print(f"âœ… CUDA Available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"\nğŸ–¥ï¸  GPU Details:")
    print(f"   Device: {torch.cuda.get_device_name(0)}")
    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"   VRAM: {vram_gb:.1f} GB")
    print(f"   Compute Capability: {torch.cuda.get_device_capability(0)}")
else:
    print("âš ï¸  No GPU detected")

print("\nâœ… Ready to start!")
print("="*60)
```

æ‡‰è©²çœ‹åˆ°ï¼š

```
PyTorch Version: 2.9.0+cu126
CUDA Available: True
Device: Tesla T4
VRAM: 15.8 GB
```

---

## Step 3ï¼šå…‹éš†å°ˆæ¡ˆ + å®‰è£ä¾è³´ï¼ˆColab Cell 2ï¼‰

```python
# å…‹éš†å€‰åº«
!git clone https://github.com/caizongxun/mistral-quantization-distillation.git
%cd mistral-quantization-distillation

# å®‰è£ä¾è³´ï¼ˆæœƒç”¨ Colab å„ªåŒ–ç‰ˆæœ¬ï¼‰
print("â³ Installing dependencies (this may take 2-3 minutes)...")
!pip install -q -r requirements-colab.txt

print("\nâœ… Dependencies installed!")
print("\nğŸ“‚ Current directory:")
!pwd
!ls -la
```

---

## Step 4ï¼šMistral-7B 4-bit é‡åŒ–ï¼ˆColab Cell 3ï¼‰

**é è¨ˆæ™‚é–“ï¼š10-15 åˆ†é˜**

```python
# æ­¥é©Ÿ 2.1ï¼šé‡åŒ–
print("\n" + "="*60)
print("ğŸ”¥ Step 1: Mistral-7B 4-bit Quantization")
print("="*60)

!python mistral_quantization.py \
    --model mistralai/Mistral-7B-v0.1 \
    --output models/mistral-7b-4bit \
    --device cuda

print("\nâœ… Quantization complete!")
```

æœƒçœ‹åˆ°é€²åº¦ï¼š

```
âœ… Tokenizer downloaded
âœ… Model loaded with 4-bit quantization
ğŸ’¾ After Quantization | Current: 4.1GB | Peak: 4.2GB | Diff: +4.1GB
âœ… Model, tokenizer, and metadata saved successfully
```

---

## Step 5ï¼šFP16 vs 4-bit Benchmarkï¼ˆColab Cell 4ï¼‰

**é è¨ˆæ™‚é–“ï¼š20-30 åˆ†é˜**

```python
print("\n" + "="*60)
print("ğŸ“Š Step 2: Benchmark FP16 vs 4-bit")
print("="*60)

!python benchmark.py \
    --fp16-model mistralai/Mistral-7B-v0.1 \
    --quantized models/mistral-7b-4bit \
    --output outputs

print("\nâœ… Benchmark complete!")
```

æª¢è¦–çµæœï¼š

```python
import pandas as pd

df = pd.read_csv("outputs/benchmark_results.csv")
print("\nğŸ“ˆ Benchmark Results:")
print(df.to_string(index=False))

# è¨ˆç®—å·®ç•°
fp16_speed = df[df['Model'].str.contains('FP16')]['Tokens/s'].values[0]
quant_speed = df[df['Model'].str.contains('4-bit')]['Tokens/s'].values[0]
speedup = quant_speed / fp16_speed

print(f"\nğŸš€ Speedup: {speedup:.1f}x faster with 4-bit!")
```

---

## Step 6ï¼šPhi-2 çŸ¥è­˜è’¸é¤¾è¨“ç·´ï¼ˆColab Cell 5ï¼‰

**é è¨ˆæ™‚é–“ï¼š45-90 åˆ†é˜**ï¼ˆå–æ±ºæ–¼é…ç½®ï¼‰

```python
print("\n" + "="*60)
print("ğŸ“ Step 3: Knowledge Distillation Training")
print("="*60)
print("\nâ³ This will take 45-90 minutes...\n")

!python distillation_training.py \
    --teacher mistralai/Mistral-7B-v0.1 \
    --student microsoft/phi-2 \
    --dataset databricks/databricks-dolly-15k \
    --samples 500 \
    --epochs 3 \
    --batch-size 4 \
    --lr 5e-5 \
    --output models/phi-2-distilled

print("\nâœ… Distillation training complete!")
```

è¨“ç·´æœƒé¡¯ç¤ºï¼š

```
ğŸ‘¨â€ğŸ« Loading Teacher Model (Mistral-7B)...
ğŸ‘©â€ğŸ’» Loading Student Model (Phi-2)...
ğŸ“š Dataset prepared: 500 samples
ğŸ”„ Training student model...
Epoch 1/3: Loss 2.34
Epoch 2/3: Loss 1.89
Epoch 3/3: Loss 1.67
âœ… Training completed in 1234.5s
```

---

## Step 7ï¼šä¸‰æ¨¡å‹æ¨ç†å°æ¯”ï¼ˆColab Cell 6ï¼‰

**é è¨ˆæ™‚é–“ï¼š5-10 åˆ†é˜**

```python
print("\n" + "="*60)
print("ğŸ§  Step 4: Inference Comparison")
print("="*60)

# å–®æ¬¡æ¸¬è©¦
test_prompt = "What is artificial intelligence?"
print(f"\nğŸ“ Test Prompt: {test_prompt}\n")

!python inference_comparison.py \
    --prompt "$test_prompt" \
    --fp16-model mistralai/Mistral-7B-v0.1 \
    --quantized models/mistral-7b-4bit \
    --distilled models/phi-2-distilled

print("\nâœ… Inference comparison complete!")
```

æœƒçœ‹åˆ°ï¼š

```
[Mistral FP16] Time: 0.80s | Tokens/s: 12.4 â†’ Response...
[Mistral 4-bit] Time: 0.26s | Tokens/s: 38.6 â†’ Response...
[Distilled Phi-2] Time: 0.35s | Tokens/s: 25.2 â†’ Response...
```

---

## Step 8ï¼šå•Ÿå‹•äº’å‹• Gradio Demoï¼ˆColab Cell 7ï¼‰

**æ°¸é åœ¨ç·šï¼ˆç›´åˆ°é—œé–‰ Notebookï¼‰**

```python
print("\n" + "="*60)
print("ğŸ¨ Step 5: Launch Gradio Demo")
print("="*60)

import subprocess
import time

print("\nâ³ Starting Gradio app...\n")

# åœ¨å¾Œå°å•Ÿå‹•
process = subprocess.Popen(
    ['python', 'app.py'],
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE
)

time.sleep(5)

print("\n" + "="*60)
print("âœ… Gradio app is running!")
print("="*60)
print("\nğŸŒ Open the link below in a new tab:")
print("   The interface will auto-appear in output below")
print("\nğŸ’¡ You can now:")
print("   1. Enter prompts in the text box")
print("   2. See all 3 models respond in parallel")
print("   3. Compare speed/memory/quality")
print("   4. Chat history saved to outputs/chat_history.csv")
print("\nâ¹ï¸  To stop: Execute cell below")
```

---

## Step 9ï¼šä¸‹è¼‰çµæœåˆ°æœ¬åœ°ï¼ˆColab Cell 8ï¼‰

```python
print("\n" + "="*60)
print("ğŸ’¾ Download Results")
print("="*60)

from google.colab import files
import shutil

print("\nğŸ“¦ Preparing download package...\n")

# å»ºç«‹ä¸‹è¼‰è³‡æ–™å¤¾
!mkdir -p colab_results
!cp -r models/mistral-7b-4bit colab_results/ 2>/dev/null || echo "Mistral 4-bit ready"
!cp -r models/phi-2-distilled colab_results/ 2>/dev/null || echo "Phi-2 distilled ready"
!cp outputs/benchmark_results.csv colab_results/ 2>/dev/null || echo "Benchmark results ready"
!cp outputs/chat_history.csv colab_results/ 2>/dev/null || echo "Chat history ready"

print("\nâœ… Creating archive...\n")
!cd colab_results && du -sh * 2>/dev/null || echo ""

print("\nğŸ“¥ Download starting...")
print("\nï¼ˆå¦‚æœæ²’æœ‰è‡ªå‹•ä¸‹è¼‰ï¼Œé»ä¸‹é¢çš„é€£çµï¼‰\n")

# ç›´æ¥ä¸‹è¼‰é‡è¦æª”æ¡ˆ
print("ğŸ“„ Downloading individual files:\n")

try:
    files.download('outputs/benchmark_results.csv')
    print("âœ… benchmark_results.csv")
except:
    print("â­ï¸  benchmark_results.csv not found yet")

try:
    files.download('outputs/chat_history.csv')
    print("âœ… chat_history.csv")
except:
    print("â­ï¸  chat_history.csv not found yet")

print("\nğŸ’¡ æ¨¡å‹æª”æ¡ˆå¯èƒ½å¾ˆå¤§ï¼Œå¦‚éœ€ä¸‹è¼‰ï¼š")
print("   1. å³ä¸Šè§’ä¸‰é»é¸ã€ä¸‹è¼‰å…¨éƒ¨ã€")
print("   2. æˆ–æ‰‹å‹•åœ¨æª”æ¡ˆç€è¦½å™¨é¸å–")
print("   3. æˆ–ç”¨ Google Drive åŒæ­¥")
```

---

## å®Œæ•´ One-Cell ç‰ˆæœ¬ï¼ˆå¿«é€Ÿæ–¹æ¡ˆï¼‰

å¦‚æœæƒ³ä¸€å€‹ cell åŸ·è¡Œæ‰€æœ‰æ­¥é©Ÿï¼š

```python
print("\nğŸš€ STARTING COMPLETE PIPELINE\n")

# 1. Clone & Install
!git clone https://github.com/caizongxun/mistral-quantization-distillation.git
%cd mistral-quantization-distillation
!pip install -q -r requirements-colab.txt

# 2. Quantize
print("\n--- STEP 1: Quantization ---")
!python mistral_quantization.py --output models/mistral-7b-4bit --device cuda

# 3. Benchmark
print("\n--- STEP 2: Benchmark ---")
!python benchmark.py --quantized models/mistral-7b-4bit --output outputs

print("\n" + "="*60)
print("âœ… PIPELINE COMPLETE!")
print("="*60)

import pandas as pd
df = pd.read_csv("outputs/benchmark_results.csv")
print("\nğŸ“Š Results:")
print(df)
```

---

## å¸¸è¦‹éŒ¯èª¤èˆ‡è§£æ±º

### âŒ `torch==2.1.2 not found`

**è§£æ±º**ï¼šrequirements å·²æ›´æ–°ç”¨ `>=2.4.0`ï¼Œè‡ªå‹•ç”¨ Colab çš„æœ€æ–°ç‰ˆæœ¬

```python
!pip install --upgrade -r requirements-colab.txt
```

### âŒ `CUDA out of memory`

**è§£æ±º**ï¼š

```python
# æ¸…ç©º GPU è¨˜æ†¶é«”
import torch
torch.cuda.empty_cache()

# æˆ–é™ä½ batch size
!python distillation_training.py --batch-size 2 --samples 200
```

### âŒ Model download hangs

**è§£æ±º**ï¼š

```python
# ç™»å…¥ Hugging Faceï¼ˆå¦‚éœ€é©—è­‰ï¼‰
from huggingface_hub import login
login(token="hf_xxxxx")  # å¾ https://huggingface.co/settings/tokens å–å¾—
```

### âŒ Colab é€£ç·šæ–·é–‹

**é é˜²**ï¼š

- å•Ÿç”¨ã€ŒKeep session aliveã€ï¼ˆå³ä¸Šè§’ï¼‰
- æ¨¡å‹æœƒæ¯æ¬¡é‡æ–°ä¸‹è¼‰ï¼ˆç”¨ cacheï¼‰
- å®šæ™‚ä¸‹è¼‰çµæœ

---

## é æœŸæ™‚é–“è¡¨

| Step | ä»»å‹™ | æ™‚é–“ | VRAM |
|------|------|------|------|
| 1 | Setup | 2 min | - |
| 2 | Quantize | 10 min | 4.1 GB |
| 3 | Benchmark | 25 min | 16 GB |
| 4 | Distill | 60 min | 14 GB |
| 5 | Inference | 5 min | 4 GB |
| 6 | Demo | unlimited | 4 GB |
| **Total** | **å®Œæ•´ç®¡ç·š** | **~2 å°æ™‚** | **16 GB** |

---

## é¡å¤–è³‡æº

- ğŸ”— [å®Œæ•´ Notebook](https://colab.research.google.com/github/caizongxun/mistral-quantization-distillation/blob/main/colab_full_pipeline.ipynb)
- ğŸ“– [é‡åŒ–æ·±å…¥è¬›è§£](docs/QUANTIZATION.md)
- ğŸ“ [è’¸é¤¾åŸç†](docs/DISTILLATION.md)
- ğŸ› [æ•…éšœæ’æŸ¥](docs/TROUBLESHOOTING.md)

---

## ä¸‹ä¸€æ­¥

1. âœ… å®Œæˆ Colab ä¸Šçš„æ‰€æœ‰æ­¥é©Ÿ
2. ğŸ“¥ ä¸‹è¼‰é‡åŒ–æ¨¡å‹ + è’¸é¤¾æ¨¡å‹
3. ğŸ  è§£å£“åˆ°æœ¬åœ°å°ˆæ¡ˆè³‡æ–™å¤¾
4. ğŸš€ æœ¬åœ°åŸ·è¡Œ `python app.py` ä½¿ç”¨

ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼ ğŸ‰
