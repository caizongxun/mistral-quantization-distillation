{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Mistral-7B Quantization & Distillation in Google Colab\n",
    "\n",
    "Complete pipeline for:\n",
    "1. Mistral-7B 4-bit quantization\n",
    "2. Benchmark FP16 vs 4-bit\n",
    "3. Phi-2 knowledge distillation\n",
    "4. Interactive Gradio demo\n",
    "\n",
    "> **Note**: Use GPU runtime for fastest results (Edit > Notebook settings > T4 GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader 2>/dev/null || echo 'No GPU detected - CPU mode active'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/caizongxun/mistral-quantization-distillation.git\n",
    "%cd mistral-quantization-distillation\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements-colab.txt\n",
    "\n",
    "# Verify PyTorch\n",
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Mistral-7B 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quantization\n",
    "!python mistral_quantization.py --output ./models/mistral-7b-4bit --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify quantized model was saved\n",
    "import os\n",
    "model_path = './models/mistral-7b-4bit'\n",
    "if os.path.exists(model_path):\n",
    "    files = os.listdir(model_path)\n",
    "    print(f'âœ… Quantized model saved with {len(files)} files:')\n",
    "    for f in files[:5]:\n",
    "        print(f'  - {f}')\n",
    "    if len(files) > 5:\n",
    "        print(f'  ... and {len(files)-5} more files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Benchmark FP16 vs 4-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark\n",
    "!python benchmark.py --fp16-model mistralai/Mistral-7B-v0.1 --quantized ./models/mistral-7b-4bit --output ./outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display benchmark results\n",
    "import pandas as pd\n",
    "\n",
    "results_path = './outputs/benchmark_results.csv'\n",
    "if os.path.exists(results_path):\n",
    "    df = pd.read_csv(results_path)\n",
    "    print('ðŸ“Š Benchmark Results:')\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Phi-2 Knowledge Distillation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab with limited time, use fewer samples and epochs\n",
    "!python distillation_training.py \\\n",
    "    --teacher mistralai/Mistral-7B-v0.1 \\\n",
    "    --student microsoft/phi-2 \\\n",
    "    --dataset databricks/databricks-dolly-15k \\\n",
    "    --samples 500 \\\n",
    "    --epochs 3 \\\n",
    "    --batch-size 4 \\\n",
    "    --output ./models/phi-2-distilled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify distilled model\n",
    "distilled_path = './models/phi-2-distilled'\n",
    "if os.path.exists(distilled_path):\n",
    "    files = os.listdir(distilled_path)\n",
    "    print(f'âœ… Distilled model saved with {len(files)} files')\n",
    "    import json\n",
    "    if os.path.exists(os.path.join(distilled_path, 'distillation_metadata.json')):\n",
    "        with open(os.path.join(distilled_path, 'distillation_metadata.json')) as f:\n",
    "            metadata = json.load(f)\n",
    "            print(f'Training dataset: {metadata.get(\"training_dataset\")}')\n",
    "            print(f'Training time: {metadata.get(\"training_time_seconds\"):.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Inference on All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single prompt with all models\n",
    "!python inference_comparison.py --prompt 'What is artificial intelligence?' --interactive false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Launch Interactive Gradio Demo\n",
    "\n",
    "> **Note**: Gradio in Colab will provide a public shareable URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Gradio app with Colab integration\n",
    "from gradio.components import Textbox\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start app in background\n",
    "process = subprocess.Popen(['python', 'app.py'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "print('â³ Waiting for Gradio to start...')\n",
    "time.sleep(5)\n",
    "\n",
    "print('âœ… Gradio app is running!')\n",
    "print('ðŸ“± The interface is available at http://localhost:7860')\n",
    "print('ðŸ’¡ In Colab, click the generated public URL link')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare files for download\n",
    "import shutil\n",
    "\n",
    "# Create download package\n",
    "!mkdir -p download_package\n",
    "!cp -r models/mistral-7b-4bit download_package/ 2>/dev/null || echo 'Quantized model ready'\n",
    "!cp -r models/phi-2-distilled download_package/ 2>/dev/null || echo 'Distilled model ready'\n",
    "!cp outputs/benchmark_results.csv download_package/ 2>/dev/null || echo 'Benchmark results ready'\n",
    "!cp outputs/chat_history.csv download_package/ 2>/dev/null || echo 'Chat history ready'\n",
    "\n",
    "# Compress\n",
    "!cd download_package && du -sh * 2>/dev/null || echo 'Package ready for download'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final summary\n",
    "print('\\n' + '='*60)\n",
    "print('ðŸŽ‰ PIPELINE COMPLETE!')\n",
    "print('='*60)\n",
    "print('\\nâœ… Generated Artifacts:')\n",
    "print('  1. Mistral-7B 4-bit Quantized: ./models/mistral-7b-4bit/')\n",
    "print('  2. Phi-2 Distilled: ./models/phi-2-distilled/')\n",
    "print('  3. Benchmark Results: ./outputs/benchmark_results.csv')\n",
    "print('  4. Chat History: ./outputs/chat_history.csv')\n",
    "print('\\nðŸ“Š Next Steps:')\n",
    "print('  - Download models and results from ./download_package/')\n",
    "print('  - Use quantized models in your own applications')\n",
    "print('  - Fine-tune distilled model on custom data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
