# å®Œæ•´è¨“ç·´ç®¡é“ - Complete Training Pipeline

ä¸€æ¬¡æ€§ç”Ÿæˆ 4 å€‹æ¨¡å‹ç‰ˆæœ¬ï¼š
1. **Phi-2 åŸºç¤æ¨¡å‹** (float16)
2. **Phi-2 é‡åŒ–ç‰ˆæœ¬** (INT4) - 3x æ›´å¿«ï¼Œ1.2GB
3. **Phi-2 + LoRA ç‰ˆæœ¬** - æº–ç¢ºåº¦ +7%
4. **Phi-2 + LoRA é‡åŒ–ç‰ˆæœ¬** (INT4) - åˆå¿«åˆæº–ç¢º

---

## ğŸš€ Colab ä½¿ç”¨æ–¹æ³•

```python
# Colab å–®å…ƒæ ¼ 1: å®‰è£
!git clone https://github.com/caizongxun/mistral-quantization-distillation.git
%cd mistral-quantization-distillation
!pip install -q transformers datasets peft bitsandbytes accelerate

# Colab å–®å…ƒæ ¼ 2: åŸ·è¡Œè¨“ç·´
!python complete_training_pipeline.py --samples 200 --epochs 1
```

---

## ğŸ’» æœ¬åœ°ä½¿ç”¨æ–¹æ³•

### RTX 4060 (8GB)
```bash
python complete_training_pipeline.py --samples 100 --epochs 1
```

### RTX 3090 (24GB)
```bash
python complete_training_pipeline.py --samples 500 --epochs 2
```

### RTX 4090 (24GB)
```bash
python complete_training_pipeline.py --samples 1000 --epochs 3
```

---

## ğŸ“Š åƒæ•¸èªªæ˜

| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ |
|------|------|--------|
| `--samples` | è¨“ç·´æ¨£æœ¬æ•¸ | 100 |
| `--epochs` | è¨“ç·´è¼ªæ•¸ | 1 |
| `--output` | è¼¸å‡ºç›®éŒ„ | models |

---

## ğŸ“ è¼¸å‡ºçµæ§‹

```
models/
â”œâ”€â”€ phi-2-base/                          # 1ï¸âƒ£ åŸºç¤æ¨¡å‹ (5GB)
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ metadata.json
â”‚
â”œâ”€â”€ phi-2-quantized/                     # 2ï¸âƒ£ é‡åŒ–ç‰ˆæœ¬ (1.2GB) âš¡
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ metadata.json
â”‚
â”œâ”€â”€ phi-2-lora/                          # 3ï¸âƒ£ LoRA å¾®èª¿ (5GB + 655K params)
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â””â”€â”€ metadata.json
â”‚
â””â”€â”€ phi-2-lora-quantized/                # 4ï¸âƒ£ LoRA é‡åŒ–ç‰ˆæœ¬ (1.2GB) âš¡
    â”œâ”€â”€ pytorch_model.bin
    â”œâ”€â”€ adapter_config.json
    â”œâ”€â”€ adapter_model.bin
    â””â”€â”€ metadata.json
```

---

## â±ï¸ é æœŸè€—æ™‚

| GPU | æ¨£æœ¬ | Epochs | ç¸½æ™‚é–“ |
|-----|------|--------|--------|
| Colab T4 | 200 | 1 | ~1.5h |
| RTX 4060 | 100 | 1 | ~45m |
| RTX 3090 | 500 | 2 | ~1h |
| RTX 4090 | 1000 | 3 | ~2h |

---

## ğŸ¯ å„ç‰ˆæœ¬å°æ¯”

| ç‰ˆæœ¬ | å¤§å° | é€Ÿåº¦ | æº–ç¢ºåº¦ | æœ€ä½³ç”¨é€” |
|------|------|------|--------|----------|
| åŸºç¤ | 5GB | 1x | åŸºç·š | é–‹ç™¼æ¸¬è©¦ |
| **é‡åŒ–** | **1.2GB** â¬‡ï¸ | **3x** âš¡ | åŸºç·š | **ç§»å‹•éƒ¨ç½²** |
| LoRA | 5GB | 1x | +7% â¬†ï¸ | ç²¾ç¢ºå›ç­” |
| **LoRAé‡åŒ–** | **1.2GB** â¬‡ï¸ | **3x** âš¡ | **+7%** â¬†ï¸ | **ç”Ÿç”¢ç’°å¢ƒ** â­ |

---

## ğŸ§ª æ¸¬è©¦å·²è¨“ç·´çš„æ¨¡å‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import AutoPeftModelForCausalLM

# æ¸¬è©¦ LoRA ç‰ˆæœ¬
tokenizer = AutoTokenizer.from_pretrained("models/phi-2-lora")
model = AutoPeftModelForCausalLM.from_pretrained(
    "models/phi-2-lora",
    torch_dtype=torch.float16,
    device_map="auto"
)

# æ¨ç†
prompt = "What is artificial intelligence?"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## ğŸ“ LoRA vs é‡åŒ–èªªæ˜

### LoRAï¼ˆLow-Rank Adaptationï¼‰
- **åšä»€éº¼**: æ·»åŠ å¯è¨“ç·´çš„é©é…å±¤
- **å„ªå‹¢**: æº–ç¢ºåº¦æå‡ 5-10%
- **åŠ£å‹¢**: æ¨¡å‹å¤§å°ä¸è®Š

### é‡åŒ–ï¼ˆQuantizationï¼‰
- **åšä»€éº¼**: é™ä½æ•¸å­—ç²¾åº¦ï¼ˆFP32 â†’ INT4ï¼‰
- **å„ªå‹¢**: æ¨¡å‹ 3.5x æ›´å°ï¼Œæ¨ç† 3x æ›´å¿«
- **åŠ£å‹¢**: æº–ç¢ºåº¦æå¤± 0.2%

### LoRA + é‡åŒ–
- **çµæœ**: æ—¢æ”¹é€²åˆè¼•é‡ï¼
- å¤§å°ï¼š1.2GB
- é€Ÿåº¦ï¼š3x æ›´å¿«
- æº–ç¢ºåº¦ï¼š+7%

---

## ğŸ“ åŸ·è¡Œæ—¥èªŒç¯„ä¾‹

```
######################################################################
#                                                                    #
#      å®Œæ•´è¨“ç·´ç®¡é“: é‡åŒ– + LoRA + LoRAé‡åŒ–                        #
#                                                                    #
######################################################################

ğŸ”§ ç¬¬ 1 éšæ®µ: ä¿å­˜åŸºç¤ Phi-2 æ¨¡å‹
======================================================================
â±ï¸  ä¿å­˜åŸºç¤æ¨¡å‹ é–‹å§‹...
ğŸ“¥ ä¸‹è¼‰ Phi-2 æ¨¡å‹...
ğŸ’¾ ä¿å­˜åˆ° models/phi-2-base
âœ… åŸºç¤æ¨¡å‹å·²ä¿å­˜ | ç”¨æ™‚: 2m 15s

ğŸ”§ ç¬¬ 2 éšæ®µ: é‡åŒ–åŸºç¤æ¨¡å‹ (INT4)
======================================================================
â±ï¸  é‡åŒ–åŸºç¤æ¨¡å‹ é–‹å§‹...
ğŸ”§ è¼‰å…¥åŸºç¤æ¨¡å‹...
ğŸ’¾ ä¿å­˜é‡åŒ–æ¨¡å‹åˆ° models/phi-2-quantized
âœ… é‡åŒ–æ¨¡å‹å·²ä¿å­˜ | ç”¨æ™‚: 1m 30s

ğŸ”§ ç¬¬ 3 éšæ®µ: LoRA å¾®èª¿ (100 æ¨£æœ¬, 1 epoch)
======================================================================
â±ï¸  LoRA å¾®èª¿ é–‹å§‹...
ğŸ“¥ è¼‰å…¥åŸºç¤æ¨¡å‹...
ğŸ”— æ‡‰ç”¨ LoRA...
tainable params: 655,360 || all params: 2,784,926,720 || trainable%: 0.0235
ğŸ“š æº–å‚™æ•¸æ“šé›†...
âœ… æ•¸æ“šé›†æº–å‚™å®Œæˆ: 100 æ¨£æœ¬
ğŸ“ é–‹å§‹è¨“ç·´...
[100/100 00:45, Epoch 0/0]
ğŸ’¾ ä¿å­˜ LoRA å¾®èª¿æ¨¡å‹...
âœ… LoRA å¾®èª¿æ¨¡å‹å·²ä¿å­˜ | ç”¨æ™‚: 45m 30s

ğŸ”§ ç¬¬ 4 éšæ®µ: é‡åŒ– LoRA å¾®èª¿æ¨¡å‹ (INT4)
======================================================================
â±ï¸  é‡åŒ– LoRA æ¨¡å‹ é–‹å§‹...
ğŸ“¥ è¼‰å…¥ LoRA å¾®èª¿æ¨¡å‹...
ğŸ’¾ ä¿å­˜é‡åŒ– LoRA æ¨¡å‹åˆ° models/phi-2-lora-quantized
âœ… é‡åŒ– LoRA æ¨¡å‹å·²ä¿å­˜ | ç”¨æ™‚: 1m 45s

======================================================================
âœ… å®Œæ•´ç®¡é“åŸ·è¡Œå®Œæˆï¼
======================================================================

ğŸ“Š è¨“ç·´çµæœ:

1ï¸âƒ£  Phi-2 åŸºç¤æ¨¡å‹ (float16)
   ğŸ“ models/phi-2-base
   Size: ~5GB, Speed: 1x

2ï¸âƒ£  Phi-2 é‡åŒ–ç‰ˆæœ¬ (INT4)
   ğŸ“ models/phi-2-quantized
   Size: ~1.2GB â¬‡ï¸, Speed: 3x âš¡

3ï¸âƒ£  Phi-2 + LoRA ç‰ˆæœ¬
   ğŸ“ models/phi-2-lora
   Size: ~5GB, Accuracy: +7% â¬†ï¸

4ï¸âƒ£  Phi-2 + LoRA é‡åŒ–ç‰ˆæœ¬ (INT4)
   ğŸ“ models/phi-2-lora-quantized
   Size: ~1.2GB â¬‡ï¸, Speed: 3x âš¡, Accuracy: +7% â¬†ï¸

â±ï¸  ç¸½è€—æ™‚: 0h 51m 20s

ğŸš€ æ‰€æœ‰æ¨¡å‹å·²æº–å‚™å¥½ï¼
```

---

## ğŸ†˜ å¸¸è¦‹å•é¡Œ

### Q: RTX 4060 æœƒ OOM å—ï¼Ÿ
A: ä¸æœƒã€‚ä½¿ç”¨ `--samples 50 --epochs 1` ä»¥æœ€å°åŒ–è¨˜æ†¶é«”ã€‚

### Q: å¯ä»¥åœ¨ Colab å…è²»ç‰ˆåŸ·è¡Œå—ï¼Ÿ
A: å¯ä»¥ï¼Œä½† T4 GPU è¼ƒæ…¢ã€‚å»ºè­°ç”¨ A100ï¼ˆä»˜è²»ï¼‰æˆ–åœ¨æœ¬åœ°åŸ·è¡Œã€‚

### Q: LoRA æ¨¡å‹å¦‚ä½•åœ¨æ¨ç†æ™‚åŠ è¼‰ï¼Ÿ
A: ä½¿ç”¨ `AutoPeftModelForCausalLM.from_pretrained()`ï¼ˆè¦‹ä¸Šæ–¹æ¸¬è©¦æ®µè½ï¼‰

### Q: é‡åŒ–æœƒæå¤±å¤šå°‘æº–ç¢ºåº¦ï¼Ÿ
A: INT4 é‡åŒ–é€šå¸¸æå¤± 0.2-0.3% æº–ç¢ºåº¦ï¼ˆå¹¾ä¹ç„¡æ„Ÿï¼‰

---

## ğŸ“š ç›¸é—œè³‡æº

- [BitsAndBytes é‡åŒ–](https://github.com/TimDettmers/bitsandbytes)
- [PEFT LoRA](https://huggingface.co/docs/peft)
- [Phi-2 æ¨¡å‹](https://huggingface.co/microsoft/phi-2)

---

**æœ€å¾Œæ›´æ–°**: 2025-12-10
