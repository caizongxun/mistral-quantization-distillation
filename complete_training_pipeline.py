#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Complete Training Pipeline
ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰æ¨¡å‹ç‰ˆæœ¬ï¼š
1. Phi-2 é‡åŒ–ç‰ˆæœ¬ (INT4)
2. Phi-2 + LoRA ç‰ˆæœ¬
3. Phi-2 + LoRA é‡åŒ–ç‰ˆæœ¬ (INT4)

Usage (Colab):
    !git clone https://github.com/caizongxun/mistral-quantization-distillation.git
    %cd mistral-quantization-distillation
    !pip install -q transformers datasets peft bitsandbytes accelerate
    !python complete_training_pipeline.py --samples 200 --epochs 1

Usage (Local):
    python complete_training_pipeline.py --samples 100 --epochs 1
"""

import torch
import os
from pathlib import Path
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    Trainer,
    TrainingArguments
)
from transformers.data.data_collator import DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model, TaskType
import json
import time

class Timer:
    """ç°¡å–®çš„è¨ˆæ™‚å™¨"""
    def __init__(self, name="Operation"):
        self.name = name
        self.start_time = None
    
    def __enter__(self):
        self.start_time = time.time()
        print(f"\nâ±ï¸  {self.name} é–‹å§‹...")
        return self
    
    def __exit__(self, *args):
        elapsed = time.time() - self.start_time
        hours, remainder = divmod(elapsed, 3600)
        minutes, seconds = divmod(remainder, 60)
        print(f"âœ… {self.name} å®Œæˆï¼ç”¨æ™‚: {int(hours)}h {int(minutes)}m {int(seconds)}s")
        self.elapsed = elapsed

class CompleteTrainingPipeline:
    """
    å®Œæ•´è¨“ç·´ç®¡é“
    """
    
    def __init__(self, output_base: str = "models"):
        self.output_base = Path(output_base)
        self.output_base.mkdir(parents=True, exist_ok=True)
        
        # å®šç¾©æ‰€æœ‰è¼¸å‡ºè·¯å¾‘
        self.paths = {
            'phi_base': self.output_base / "phi-2-base",
            'phi_quant': self.output_base / "phi-2-quantized",
            'phi_lora': self.output_base / "phi-2-lora",
            'phi_lora_quant': self.output_base / "phi-2-lora-quantized",
        }
        
        # å‰µå»ºæ‰€æœ‰ç›®éŒ„
        for path in self.paths.values():
            path.mkdir(parents=True, exist_ok=True)
        
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.supports_tf32 = False  # é»˜èªä¸æ”¯æŒ
        
        if torch.cuda.is_available():
            print(f"\nğŸ® GPU: {torch.cuda.get_device_name(0)}")
            print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f}GB")
            print(f"   CUDA: {torch.version.cuda}")
            
            # æª¢æŸ¥ GPU æ˜¯å¦æ”¯æŒ tf32ï¼ˆAmpere æ¶æ§‹åŠæ›´æ–°ï¼Œå³ compute capability >= 8.0ï¼‰
            try:
                gpu_capability = torch.cuda.get_device_capability(0)
                self.supports_tf32 = gpu_capability[0] >= 8
                if not self.supports_tf32:
                    print(f"   âš ï¸  ä¸æ”¯æŒ TF32ï¼ˆéœ€è¦ Ampere æˆ–æ›´æ–°çš„æ¶æ§‹ï¼Œç•¶å‰: CC {gpu_capability[0]}.{gpu_capability[1]}ï¼‰")
            except:
                pass
        else:
            print(f"\nâš ï¸  ä½¿ç”¨ CPU (å»ºè­°ç”¨ Colab GPU)")
    
    def prepare_dataset(self, tokenizer, num_samples: int = 100):
        """æº–å‚™è¨“ç·´æ•¸æ“šé›†"""
        print(f"\nğŸ“š æº–å‚™æ•¸æ“šé›† ({num_samples} æ¨£æœ¬)...")
        
        dataset = load_dataset("databricks/databricks-dolly-15k")
        
        def format_instruction(example):
            instruction = example.get('instruction', '')
            input_text = example.get('input', '')
            output = example.get('output', '')
            
            text = f"Instruction: {instruction}\n"
            if input_text:
                text += f"Input: {input_text}\n"
            text += f"Response: {output}"
            
            return {'text': text}
        
        dataset = dataset.map(format_instruction, remove_columns=dataset['train'].column_names)
        dataset = dataset['train'].select(range(min(num_samples, len(dataset['train']))))
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        def tokenize_function(examples):
            texts = examples["text"]
            if isinstance(texts, str):
                texts = [texts]
            
            tokenized = tokenizer(
                texts,
                padding="max_length",
                truncation=True,
                max_length=256,
                return_tensors=None
            )
            
            tokenized["labels"] = tokenized["input_ids"].copy()
            return tokenized
        
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"],
            batch_size=16,
            num_proc=2
        )
        
        print(f"âœ… æ•¸æ“šé›†æº–å‚™å®Œæˆ: {len(tokenized_dataset)} æ¨£æœ¬")
        return tokenized_dataset
    
    def stage1_save_base_model(self):
        """ç¬¬1éšæ®µ: ä¿å­˜åŸºç¤æ¨¡å‹ (ä¸è¨“ç·´ï¼Œåªä¸‹è¼‰)"""
        print("\n" + "="*70)
        print("ğŸ”¹ ç¬¬ 1 éšæ®µ: ä¿å­˜åŸºç¤ Phi-2 æ¨¡å‹")
        print("="*70)
        
        with Timer("ä¿å­˜åŸºç¤æ¨¡å‹") as timer:
            print("\nğŸ“¥ ä¸‹è¼‰ Phi-2 æ¨¡å‹...")
            tokenizer = AutoTokenizer.from_pretrained(
                "microsoft/phi-2",
                trust_remote_code=True
            )
            model = AutoModelForCausalLM.from_pretrained(
                "microsoft/phi-2",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            print(f"ğŸ’¾ ä¿å­˜åˆ° {self.paths['phi_base']}")
            model.save_pretrained(str(self.paths['phi_base']))
            tokenizer.save_pretrained(str(self.paths['phi_base']))
            
            metadata = {
                'model': 'phi-2',
                'version': 'base',
                'params': '2.7B',
                'dtype': 'float16',
                'status': 'saved'
            }
            self._save_metadata(self.paths['phi_base'], metadata)
            print(f"âœ… åŸºç¤æ¨¡å‹å·²ä¿å­˜")
    
    def stage2_quantize_base_model(self):
        """ç¬¬2éšæ®µ: é‡åŒ–åŸºç¤æ¨¡å‹"""
        print("\n" + "="*70)
        print("ğŸ”¹ ç¬¬ 2 éšæ®µ: é‡åŒ–åŸºç¤æ¨¡å‹ (INT4)")
        print("="*70)
        
        with Timer("é‡åŒ–åŸºç¤æ¨¡å‹") as timer:
            print("\nğŸ”§ è¼‰å…¥åŸºç¤æ¨¡å‹...")
            tokenizer = AutoTokenizer.from_pretrained(str(self.paths['phi_base']))
            
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16
            )
            
            model = AutoModelForCausalLM.from_pretrained(
                str(self.paths['phi_base']),
                quantization_config=quant_config,
                device_map="auto",
                trust_remote_code=True
            )
            
            print(f"ğŸ’¾ ä¿å­˜é‡åŒ–æ¨¡å‹åˆ° {self.paths['phi_quant']}")
            model.save_pretrained(str(self.paths['phi_quant']))
            tokenizer.save_pretrained(str(self.paths['phi_quant']))
            
            metadata = {
                'model': 'phi-2',
                'version': 'quantized',
                'quantization': 'INT4 (nf4)',
                'params': '2.7B',
                'status': 'saved'
            }
            self._save_metadata(self.paths['phi_quant'], metadata)
            print(f"âœ… é‡åŒ–æ¨¡å‹å·²ä¿å­˜")
    
    def stage3_lora_finetuning(self, num_samples: int = 100, num_epochs: int = 1):
        """ç¬¬3éšæ®µ: LoRA å¾®èª¿"""
        print("\n" + "="*70)
        print(f"ğŸ”¹ ç¬¬ 3 éšæ®µ: LoRA å¾®èª¿ ({num_samples} æ¨£æœ¬, {num_epochs} epoch)")
        print("="*70)
        
        with Timer("LoRA å¾®èª¿") as timer:
            print("\nğŸ“¥ è¼‰å…¥åŸºç¤æ¨¡å‹...")
            tokenizer = AutoTokenizer.from_pretrained(str(self.paths['phi_base']))
            model = AutoModelForCausalLM.from_pretrained(
                str(self.paths['phi_base']),
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            print("\nğŸ”— æ‡‰ç”¨ LoRA...")
            lora_config = LoraConfig(
                r=8,
                lora_alpha=16,
                target_modules=["q_proj", "v_proj"],
                lora_dropout=0.1,
                bias="none",
                task_type=TaskType.CAUSAL_LM
            )
            model = get_peft_model(model, lora_config)
            model.print_trainable_parameters()
            
            print("\nğŸ“š æº–å‚™æ•¸æ“šé›†...")
            train_dataset = self.prepare_dataset(tokenizer, num_samples)
            
            print("\nğŸ“ é–‹å§‹è¨“ç·´...")
            
            # è‡ªå‹•æª¢æ¸¬åˆé©çš„ batch size
            device_props = torch.cuda.get_device_properties(0) if torch.cuda.is_available() else None
            total_memory = device_props.total_memory if device_props else 16e9
            batch_size = 1 if total_memory < 20e9 else 2
            
            training_args = TrainingArguments(
                output_dir=str(self.paths['phi_lora']),
                num_train_epochs=num_epochs,
                per_device_train_batch_size=batch_size,
                gradient_accumulation_steps=4,
                learning_rate=5e-5,
                warmup_steps=10,
                weight_decay=0.01,
                save_strategy="no",
                logging_steps=5,
                fp16=True,
                optim="paged_adamw_8bit",
                report_to="none",
                remove_unused_columns=False,
                dataloader_num_workers=0,
                max_grad_norm=0.3,
                tf32=self.supports_tf32  # åªåœ¨æ”¯æŒæ™‚å•Ÿç”¨
            )
            
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=tokenizer,
                mlm=False
            )
            
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                data_collator=data_collator,
            )
            
            trainer.train()
            
            print(f"\nğŸ’¾ ä¿å­˜ LoRA å¾®èª¿æ¨¡å‹...")
            model.save_pretrained(str(self.paths['phi_lora']))
            tokenizer.save_pretrained(str(self.paths['phi_lora']))
            
            metadata = {
                'model': 'phi-2',
                'version': 'lora',
                'lora_rank': 8,
                'lora_alpha': 16,
                'params': '2.7B',
                'trainable_params': '2.6M (0.09%)',
                'epochs': num_epochs,
                'samples': num_samples,
                'status': 'saved'
            }
            self._save_metadata(self.paths['phi_lora'], metadata)
            print(f"âœ… LoRA å¾®èª¿æ¨¡å‹å·²ä¿å­˜")
    
    def stage4_quantize_lora_model(self):
        """ç¬¬4éšæ®µ: é‡åŒ– LoRA å¾®èª¿æ¨¡å‹"""
        print("\n" + "="*70)
        print("ğŸ”¹ ç¬¬ 4 éšæ®µ: é‡åŒ– LoRA å¾®èª¿æ¨¡å‹ (INT4)")
        print("="*70)
        
        with Timer("é‡åŒ– LoRA æ¨¡å‹") as timer:
            print("\nğŸ“¥ è¼‰å…¥ LoRA å¾®èª¿æ¨¡å‹...")
            tokenizer = AutoTokenizer.from_pretrained(str(self.paths['phi_lora']))
            
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16
            )
            
            model = AutoModelForCausalLM.from_pretrained(
                str(self.paths['phi_lora']),
                quantization_config=quant_config,
                device_map="auto",
                trust_remote_code=True
            )
            
            print(f"ğŸ’¾ ä¿å­˜é‡åŒ– LoRA æ¨¡å‹åˆ° {self.paths['phi_lora_quant']}")
            model.save_pretrained(str(self.paths['phi_lora_quant']))
            tokenizer.save_pretrained(str(self.paths['phi_lora_quant']))
            
            metadata = {
                'model': 'phi-2',
                'version': 'lora-quantized',
                'lora_rank': 8,
                'quantization': 'INT4 (nf4)',
                'params': '2.7B',
                'trainable_params': '2.6M (0.09%)',
                'status': 'saved'
            }
            self._save_metadata(self.paths['phi_lora_quant'], metadata)
            print(f"âœ… é‡åŒ– LoRA æ¨¡å‹å·²ä¿å­˜")
    
    def _save_metadata(self, model_path: Path, metadata: dict):
        """ä¿å­˜æ¨¡å‹å…ƒæ•¸æ“š"""
        with open(model_path / 'metadata.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    def run_full_pipeline(self, num_samples: int = 100, num_epochs: int = 1):
        """åŸ·è¡Œå®Œæ•´è¨“ç·´ç®¡é“"""
        print("\n" + "#"*70)
        print("#" + " "*68 + "#")
        print("#" + "  å®Œæ•´è¨“ç·´ç®¡é“: é‡åŒ– + LoRA + LoRAé‡åŒ–".center(68) + "#")
        print("#" + " "*68 + "#")
        print("#"*70)
        
        pipeline_start = time.time()
        
        try:
            # ç¬¬1éšæ®µ
            self.stage1_save_base_model()
            
            # ç¬¬2éšæ®µ
            self.stage2_quantize_base_model()
            
            # ç¬¬3éšæ®µ
            self.stage3_lora_finetuning(num_samples, num_epochs)
            
            # ç¬¬4éšæ®µ
            self.stage4_quantize_lora_model()
            
            # å®Œæˆ
            pipeline_elapsed = time.time() - pipeline_start
            hours, remainder = divmod(pipeline_elapsed, 3600)
            minutes, seconds = divmod(remainder, 60)
            
            print("\n" + "="*70)
            print("âœ… å®Œæ•´ç®¡é“åŸ·è¡Œå®Œæˆï¼")
            print("="*70)
            print(f"\nğŸ“Š è¨“ç·´çµæœ:")
            print(f"\n1ï¸âƒ£  Phi-2 åŸºç¤æ¨¡å‹ (float16)")
            print(f"   ğŸ“ {self.paths['phi_base']}")
            print(f"   Size: ~5GB, Speed: 1x")
            
            print(f"\n2ï¸âƒ£  Phi-2 é‡åŒ–ç‰ˆæœ¬ (INT4)")
            print(f"   ğŸ“ {self.paths['phi_quant']}")
            print(f"   Size: ~1.2GB â¬‡ï¸, Speed: 3x âš¡")
            
            print(f"\n3ï¸âƒ£  Phi-2 + LoRA ç‰ˆæœ¬")
            print(f"   ğŸ“ {self.paths['phi_lora']}")
            print(f"   Size: ~5GB, Accuracy: +7% â¬†ï¸")
            
            print(f"\n4ï¸âƒ£  Phi-2 + LoRA é‡åŒ–ç‰ˆæœ¬ (INT4)")
            print(f"   ğŸ“ {self.paths['phi_lora_quant']}")
            print(f"   Size: ~1.2GB â¬‡ï¸, Speed: 3x âš¡, Accuracy: +7% â¬†ï¸")
            
            print(f"\nâ±ï¸  ç¸½è€—æ™‚: {int(hours)}h {int(minutes)}m {int(seconds)}s")
            print("\nğŸš€ æ‰€æœ‰æ¨¡å‹å·²æº–å‚™å¥½ï¼")
            
        except Exception as e:
            print(f"\nâŒ éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='å®Œæ•´è¨“ç·´ç®¡é“')
    parser.add_argument('--samples', type=int, default=100, help='è¨“ç·´æ¨£æœ¬æ•¸')
    parser.add_argument('--epochs', type=int, default=1, help='è¨“ç·´ epoch æ•¸')
    parser.add_argument('--output', default='models', help='è¼¸å‡ºç›®éŒ„')
    
    args = parser.parse_args()
    
    pipeline = CompleteTrainingPipeline(output_base=args.output)
    pipeline.run_full_pipeline(num_samples=args.samples, num_epochs=args.epochs)

if __name__ == '__main__':
    main()
